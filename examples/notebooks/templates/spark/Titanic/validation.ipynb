{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Validations Suites\n",
    "Use this example notebook as a \"boilerplate\" template for running validations.\n",
    "\n",
    "\n",
    "## IMPORTANT\n",
    "Be sure to commit your notebook to GitHub as part of your repository!  (To facilitate code review, you may wish to \"Restart Kernel and Clear All Outputs\" before committing the notebook to Git).\n",
    "\n",
    "## _We are here to help!_\n",
    "\n",
    "You can always **reach out to us on** the [**Great Expectations Slack Channel**](https://greatexpectations.io/slack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Context and Import Python Basics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from pyspark import SQLContext\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get('PYSPARK_PYTHON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder.appName(\"pytest-pyspark-local-notebook-validation\"). \\\n",
    "    master(\"local[2]\"). \\\n",
    "    config(\"spark.executor.memory\", \"6g\"). \\\n",
    "    config(\"spark.driver.memory\", \"6g\"). \\\n",
    "    config(\"spark.ui.showConsoleProgress\", \"false\"). \\\n",
    "    config(\"spark.sql.shuffle.partitions\", \"2\"). \\\n",
    "    config(\"spark.default.parallelism\", \"4\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    getOrCreate()\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Useful Python Utilities\n",
    "\n",
    "Also import GreatExpectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import great_expectations as ge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Repository Repository to Spark Context\n",
    "\n",
    "Also import frequently used utilities from your repository.\n",
    "\n",
    "### _Important_\n",
    "Make sure that the path to your repository archive in S3 for the `sc.addPyFile(s3_path_to_repo_zip)` call below is correct and that the contents are up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.addPyFile('s3://alex-ge-test/code-0.0.0.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(spark_context, path, delimiter):\n",
    "    return spark_context.read \\\n",
    "        .format(\"com.databricks.spark.csv\") \\\n",
    "        .option(\"delimiter\", delimiter) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(path)\n",
    "\n",
    "\n",
    "def load_parquet(spark_context, path, prefix_path=None, select_cols=None):\n",
    "    if prefix_path is None:\n",
    "        spark_parquet_read_func = spark_context.read\n",
    "    else:\n",
    "        spark_parquet_read_func = spark_context.read.option(\"basePath\", prefix_path)\n",
    "\n",
    "    if isinstance(path, list):\n",
    "        df = spark_parquet_read_func.parquet(*path)\n",
    "    else:\n",
    "        df = spark_parquet_read_func.parquet(path)\n",
    "\n",
    "    if select_cols:\n",
    "        df = df.select(*select_cols)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GreatExpectations Basics\n",
    "\n",
    "Check GreatExpections version.\n",
    "\n",
    "Import the GreatExpections `get_ge_context()` method and execute it using the standard buckets as the parameters:\n",
    "* json_s3_bucket -- stores JSON files containing the authoritative expectation suites definitions and validation results\n",
    "* html_docs_s3_bucket -- stores HTML files for displaying the expectation suites definitions and reporting their corresponding validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from repo.lib.test.great_expectations.ge_context import get_ge_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from great_expectations.data_context.types.base import DataContextConfig\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "\n",
    "class GeContext(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            json_s3_bucket,\n",
    "            html_docs_s3_bucket,\n",
    "            site_name='s3_site',\n",
    "            slack_webhook=None\n",
    "    ):\n",
    "        GeContext._validate_arguments(\n",
    "            json_s3_bucket=json_s3_bucket,\n",
    "            html_docs_s3_bucket=html_docs_s3_bucket,\n",
    "            site_name=site_name,\n",
    "            slack_webhook=slack_webhook\n",
    "        )\n",
    "        self._site_name = site_name\n",
    "        action_list = [\n",
    "            {\n",
    "                'name': 'store_validation_result',\n",
    "                'action': {\n",
    "                    'class_name': 'StoreValidationResultAction'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'store_evaluation_params',\n",
    "                'action': {\n",
    "                    'class_name': 'StoreEvaluationParametersAction'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'update_data_docs',\n",
    "                'action': {\n",
    "                    'class_name': 'UpdateDataDocsAction'\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        notify_slack_action_dict = {\n",
    "            'name': 'notify_slack',\n",
    "            'action': {\n",
    "                'class_name': 'SlackNotificationAction',\n",
    "                'slack_webhook': slack_webhook,\n",
    "                'notify_on': 'all',\n",
    "                'renderer': {\n",
    "                    'module_name': 'great_expectations.render.renderer.slack_renderer',\n",
    "                    'class_name': 'SlackRenderer'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if slack_webhook is not None:\n",
    "            action_list.append(notify_slack_action_dict)\n",
    "\n",
    "        project_config = DataContextConfig(\n",
    "            config_version=1,\n",
    "            datasources={\n",
    "                's3_files_spark_datasource': {\n",
    "                    'class_name': 'SparkDFDatasource',\n",
    "                    'data_asset_type': {\n",
    "                        'class_name': 'SparkDFDataset'\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            config_variables_file_path=None,\n",
    "            plugins_directory=None,\n",
    "            validation_operators={\n",
    "                'action_list_operator': {\n",
    "                    'class_name': 'ActionListValidationOperator',\n",
    "                    'action_list': action_list\n",
    "                }\n",
    "            },\n",
    "            stores={\n",
    "                'expectations': {\n",
    "                    'class_name': 'ExpectationsStore',\n",
    "                    'store_backend': {\n",
    "                        'class_name': 'TupleS3StoreBackend',\n",
    "                        'bucket': json_s3_bucket,\n",
    "                        'prefix': 'great_expectations/ExpectationSuites'\n",
    "                    }\n",
    "                },\n",
    "                'validations': {\n",
    "                    'class_name': 'ValidationsStore',\n",
    "                    'store_backend': {\n",
    "                        'class_name': 'TupleS3StoreBackend',\n",
    "                        'bucket': json_s3_bucket,\n",
    "                        'prefix': 'great_expectations/Validations'\n",
    "                    }\n",
    "                },\n",
    "                'evaluation_parameters': {\n",
    "                    'class_name': 'EvaluationParameterStore'\n",
    "                }\n",
    "            },\n",
    "            expectations_store_name='expectations',\n",
    "            validations_store_name='validations',\n",
    "            evaluation_parameter_store_name='evaluation_parameters',\n",
    "            data_docs_sites={\n",
    "                self._site_name: {\n",
    "                    'class_name': 'SiteBuilder',\n",
    "                    'store_backend': {\n",
    "                        'class_name': 'TupleS3StoreBackend',\n",
    "                        'bucket': html_docs_s3_bucket,\n",
    "                        'prefix': ''\n",
    "                    },\n",
    "                    'site_index_builder': {\n",
    "                        'class_name': 'DefaultSiteIndexBuilder',\n",
    "                        'show_cta_footer': True\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        ge_context = BaseDataContext(project_config=project_config)\n",
    "        self._ge_context = ge_context\n",
    "\n",
    "    def build_data_docs(self):\n",
    "        self._ge_context.build_data_docs(site_names=self._site_name)\n",
    "\n",
    "    def get_ge_context(self):\n",
    "        return self._ge_context\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_arguments(json_s3_bucket, html_docs_s3_bucket, site_name, slack_webhook):\n",
    "        if not json_s3_bucket or not isinstance(json_s3_bucket, str):\n",
    "            raise ValueError('Error: \"json_s3_bucket\" must be a non-empty string.')\n",
    "        if not html_docs_s3_bucket or not isinstance(html_docs_s3_bucket, str):\n",
    "            raise ValueError('Error: \"html_docs_s3_bucket\" must be a non-empty string.')\n",
    "        if not site_name or not isinstance(site_name, str):\n",
    "            raise ValueError('Error: \"site_name\" must be a non-empty string.')\n",
    "        if slack_webhook and not isinstance(slack_webhook, str):\n",
    "            raise ValueError('Error: \"slack_webhook\" must be either a non-empty string or entirely omitted.')\n",
    "\n",
    "def get_ge_context(json_s3_bucket, html_docs_s3_bucket, slack_webhook=None):\n",
    "    return GeContext(\n",
    "        json_s3_bucket=json_s3_bucket,\n",
    "        html_docs_s3_bucket=html_docs_s3_bucket,\n",
    "        slack_webhook=slack_webhook\n",
    "    ) \\\n",
    "        .get_ge_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class GeValidationContext(GeContext):\n",
    "    def validate(self, dataframe, expectation_suite):\n",
    "        ge_context = self.get_ge_context()\n",
    "        \n",
    "        ge_context.build_data_docs()\n",
    "\n",
    "        df_check_for_validations = dataframe\n",
    "        df_check_for_validations.persist()\n",
    "\n",
    "        expectation_suite_name_for_validations = expectation_suite\n",
    "        batch_kwargs_for_validations = {\n",
    "            'datasource': 's3_files_spark_datasource',\n",
    "            'dataset': df_check_for_validations\n",
    "        }\n",
    "        batch_for_validations = ge_context.get_batch(\n",
    "            expectation_suite_name=expectation_suite_name_for_validations,\n",
    "            batch_kwargs=batch_kwargs_for_validations\n",
    "        )\n",
    "\n",
    "        # Note: \"run_id\" needs to be a simple sortable timestamp, which can be readily generated by any pipeline runner.\n",
    "        run_id = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%S.%fZ')\n",
    "        validation_results = ge_context.run_validation_operator(\n",
    "            validation_operator_name='action_list_operator',\n",
    "            assets_to_validate=[batch_for_validations],\n",
    "            run_id=run_id\n",
    "        )\n",
    "\n",
    "        return validation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_s3_bucket = 'alex-ge-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_docs_s3_bucket = 'alex-ge-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_webhook = 'https://hooks.slack.com/services/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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_context = GeValidationContext(\n",
    "    json_s3_bucket=json_s3_bucket,\n",
    "    html_docs_s3_bucket=html_docs_s3_bucket,\n",
    "    slack_webhook=slack_webhook\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Your DataFrame Against Expectation Suite\n",
    "Use this notebook to validate your dataframe against your expectation suite (write down the name of the expectation suite below for future references):\n",
    "\n",
    "**Expectation Suite Name**: `Titanic_Expectation_Suite`\n",
    "\n",
    "You can always **reach out to us on** the [**Great Expectations Slack Channel**](https://greatexpectations.io/slack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Asset Specification\n",
    "\n",
    "Specify the S3 path to the data asset that you wish to reason about (by characterising it with expectations) in this notebook.  Then use the previously imported utilities to load this asset into a PySpark DataDrame (we also recommend printing some basic information about your dataframe).\n",
    "\n",
    "### Terminology\n",
    "We use the term \"check dataframe\" when referring to the dataframe corresponding to your data asset, because this is the dataframe, on which the various checks against what is expected will be performed in the course of building the expectation suite.  As part of this process, you may need to create additional columns (e.g., to combine existing columns), join different dataframes, and so on in order to produce a check dataframe for expectations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_asset_path = 's3a://alex-ge-test/data_assets/Titanic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = load_csv(\n",
    "    spark_context=spark,\n",
    "    path=data_asset_path,\n",
    "    delimiter=','\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_check.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_check.count(), len(df_check.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check.show(n=200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Expectation Suite Name\n",
    "\n",
    "Now specify the name for your expectation suite (this name must match the name of an expectation suite that you created previously for this data asset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_suite_name_for_validations = 'Titanic_Expectation_Suite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Validations\n",
    "\n",
    "Validate your check dataframe against the epectation suite for the correponding data asset.\n",
    "\n",
    "### Note\n",
    "Multiple batches of the data asset and multiple expectation suites are allowed by GE.  This would require modifying the above context helper classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = ge_context.validate(\n",
    "    dataframe=df_check,\n",
    "    expectation_suite=expectation_suite_name_for_validations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Success: {0}'.format(validation_results['success']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
